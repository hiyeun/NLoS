<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation</title>
  <meta name="description" content="Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation"/>
  <meta property="og:description" content="Project page for Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation"/>
  <meta property="og:url" content="https://hiyeun.github.io/NLoS/"/>
  <meta property="og:image" content="static/images/concept.png" />
  <meta property="og:image:width" content="3497"/>
  <meta property="og:image:height" content="1929"/>

  <meta name="twitter:title" content="Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation">
  <meta name="twitter:description" content="TWITTERProject page for Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation">
  <meta name="twitter:image" content="static/images/concept.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="2D radar point cloud, darting out, monocular depth estimation, non-line-of-sight, parked vehicle">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation</title>
  <link rel="icon" type="image/x-icon" href="static/images/nlos.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Radar-Based NLoS Pedestrian Localization<br>for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted<br>Point Cloud Interpretation</h1>
            <h2 class="title is-5 publlication-title">IROS 2025</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="#">Hee-Yeun Kim</a><sup>1</sup>,</span>
              <span class="author-block"><a href="#">Byeonggyu Park</a><sup>1</sup>,</span>
              <span class="author-block"><a href="#">Byonghyok Choi</a><sup>2</sup>,</span>
              <span class="author-block"><a href="#">Hansang Cho</a><sup>2</sup>,</span>
              <span class="author-block"><a href="#">Byungkwan Kim</a><sup>3</sup>,</span>
              <span class="author-block"><a href="#">Soomok Lee</a><sup>4</sup>,</span>
              <span class="author-block"><a href="#">Mingu Jeon</a><sup>1</sup>,</span>
              <span class="author-block"><a href="#">Seung-Woo Seo</a><sup>1</sup>,</span>
              <span class="author-block"><a href="#">Seong-Woo Kim</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
<!--                     <span class="author-block"><small><br><sup>1</sup>Seoul National University<small><br><sup>2</sup> Carnegie Mellon University<br>Conferance name and year</span> -->
                    <span class="author-block">
                      <small>
                        <br><sup>1</sup> Seoul National University
                        <br><sup>2</sup> Samsung Electro-Mechanics Co., Ltd.
                        <br><sup>3</sup> Chungnam National University
                        <br><sup>4</sup> Ajou University
                      </small>
                    </span>
                  </div>

                  
                         <!-- Arxiv PDF link -->
                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.10027" target="https://arxiv.org/pdf/2409.10027"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supp.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/knwoo/e2map" target="https://github.com/knwoo/e2map"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block"> -->
<!--                   <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" -->
<!--                   class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                     <i class="ai ai-arxiv"></i> -->
<!--                   </span> -->
<!--                   <span>arXiv</span> -->
<!--                 </a> -->
<!--               </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3 has-text-centered">Target Problem</h2>
          <video autoplay loop muted playsinline width="100%">
            <source src="./static/videos/target_problem.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="hero">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content has-text-centered">
        <h2 class="title is-3 has-text-centered">Target Problem</h2>
        <div style="position: relative; width: 52.8vw; height: 29.7vw; max-width: 960px; max-height: 540px; margin: auto; background: black; overflow: hidden; border-radius: 10px;">
          <iframe 
            src="https://www.youtube.com/embed/psThmJM8h_I?start=11&autoplay=0&mute=1&rel=0&modestbranding=1&controls=1"
            frameborder="0" 
            allow="encrypted-media" 
            allowfullscreen
            style="position: absolute; top: -5%; left: 0; width: 100%; height: 110%; object-fit: cover;">
          </iframe>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Proposed Method Video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Proposed Method</h2>
      <video id="tree" autoplay loop muted playsinline width="100%">
        <source src="./static/videos/nlos_video.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside parking in urban environments poses a significant challenge to road safety, particularly due to the sudden emergence of pedestrians. mmWave technology leverages diffraction and reflection to observe NLoS regions, and recent studies have demonstrated its potential for detecting obscured objects. However, existing approaches predominantly rely on predefined spatial information or assume simple wall reflections, thereby limiting their generalizability and practical applicability. A particular challenge arises in scenarios where pedestrians suddenly appear from between parked vehicles, as these parked vehicles act as temporary spatial obstructions. Furthermore, since parked vehicles are dynamic and may relocate over time, spatial information obtained from satellite maps or other predefined sources may not accurately reflect real-time road conditions, leading to erroneous sensor interpretations. To address this limitation, we propose an NLoS pedestrian localization framework that integrates monocular camera image with 2D radar point cloud (PCD) data. The proposed method initially detects parked vehicles through image segmentation, estimates depth to infer approximate spatial characteristics, and subsequently refines this information using 2D radar PCD to achieve precise spatial inference. Experimental evaluations conducted in real-world urban road environments demonstrate that the proposed approach enhances early pedestrian detection and contributes to improved road safety.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Concept</h2>
          <figure class="image is-fullwidth">
            <img src="static/images/concept.png" class="blend-img-background">
          </figure>
          <div class="level-set has-text-justified">
            <p>
              A novel radar-based non-line-of-sight (NLoS) pedestrian localization framework that enables robust detection of darting-out pedestrians near parked vehicles by integrating radar signals with camera-assisted point cloud interpretation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">System Overview</h2>
          <figure class="image is-fullwidth">
            <img src="static/images/nlos_arch.png" class="blend-img-background">
          </figure>
          <div class="level-set has-text-justified">
            <p>
              System Overview: The proposed system leverages radar and camera-based depth perception to enable Non-Line-of-Sight (NLoS) pedestrian localization in darting-out scenarios near parked vehicles. First, the camera assistant module processes a front camera image to perform vehicle segmentation and generate a depth point cloud (PCD), capturing spatial depth information. Next, the spatial inference module integrates the vehicle depth PCD with radar PCD, distinguishing between static and moving points while refining vehicle predictions. Finally, the NLoS object localization module processes the radar data to detect pedestrians hidden behind occlusions and localizes their positions. The final results are validated against BEV ground truth, demonstrating the system’s effectiveness in real-world urban environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Real-World Experimental environments</h2>
          <!-- <div class="content">
            <h2 class="title is-4">Experimental environment</h2>
            <center>
              <img src="static/images/gazebo_initial.png" class="center-image blend-img-background"/>
            </center>
            <div class="level-set has-text-justified">
              <p>
                We created a simulated environment that mirrored the real-world setting used for evaluation. We scanned the real-world setup using a 3D scanner and transferred the 3D model to the Gazebo simulator. The corresponding initial E2Map is displayed in the figure on the right.
              </p>
            </div>
          </div> -->
          
          <div class="content">
            <h2 class="title is-4">Experimental Scenarios</h2>
            <figure class="image is-fullwidth">
              <img src="static/images/three_env.png" class="center-image blend-img-background"/>
            </figure>
            <div class="level-set has-text-justified">
              <p>
                We designed three experimental scenarios, SA, SB, and SC, to evaluate the effectiveness of our proposed NLoS pedestrian localization system in darting-out situations near parked vehicles. In SA, a single pedestrian suddenly appears from behind a parked vehicle, testing the system’s ability to detect and track a pedestrian in a fully NLoS setting where direct visibility is obstructed. SB involves two pedestrians emerging sequentially from behind a parked vehicle, where the first pedestrian acts as an additional occlusion for the second pedestrian. This scenario evaluates the system’s ability to detect a pedestrian under dynamically changing occlusion conditions. SC consists of one pedestrian walking along a clear Line-of-Sight (LOS) path while another pedestrian emerges from the NLoS region, demonstrating the system's capability to simultaneously detect pedestrians in both LOS and NLoS conditions. These scenarios assess the system’s robustness in handling various occlusions caused by parked vehicles and moving pedestrians.          
            
              </div>

          <div class="content">
            <h2 class="title is-4">Quantitative Result</h2>
            <div class="content">
              <h2 class="title is-5">Spatial inference model</h2>
              <center>
                <img src="static/images/spatial_quan.png" class="center-image blend-img-background"/>
              </center>
            </div>

            <div class="content">
              <h2 class="title is-5">Pedestrian localization model</h2>
              <center>
                <img src="static/images/localization_quan.png" class="center-image blend-img-background"/>
              </center>
            </div>
          </div>


          
          <!-- <div class="content">
            <h2 class="title is-4">Qualitative Result</h2>
            <div class="content">
              <h2 class="title is-5">SA</h2>
              <center>
              <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
                  <!-- Your video here -->
                <!-- <source src="static/videos/e2map_danger_qual.mp4"
                type="video/mp4">
              </video>
              </center>
            </div> -->

            <!-- <div class="content">
              <h2 class="title is-5">SB</h2>
              <center>
              <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
                  <!-- Your video here -->
                <!-- <source src="static/videos/e2map_human_qual.mp4" -->
                <!-- type="video/mp4"> -->
              <!-- </video> -->
              <!-- </center> -->
            <!-- </div> -->

            <!-- <div class="content">
              <h2 class="title is-5">SC</h2>
              <center>
              <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
                  <!-- Your video here -->
                <!-- <source src="static/videos/e2map_door_qual.mp4"
                type="video/mp4">
              </video>
              </center>
            </div>            
          </div> -->

          <!-- <div class="content">
          <h2 class="title is-4">Qualitative Results from the Event Descriptor and Emotion Evaluator</h2>
            <div class="content">
              <h2 class="title is-5">Danger Sign</h2>
              <div class="content">
                <h2 class="title is-6">Event Images</h2>
                <center>
                  <img src="static/images/ds_events.png" class="center-image blend-img-background"/>
                </center>
              </div> -->

              <!-- <div class="content">
                <h2 class="title is-6">Event Description</h2>
                <center>
                  <img src="static/images/ds_ed_result.png" class="center-image blend-img-background"/>
                </center>
              </div>

              <div class="content">
                <h2 class="title is-6">Emotion Evaluation</h2>
                <center>
                  <img src="static/images/ds_ee_result.png" class="center-image blend-img-background"/>
                </center>
              </div>
            </div>

            <div class="content">
              <h2 class="title is-5">Human-Wall</h2>
              <div class="content">
                <h2 class="title is-6">Event Images</h2>
                <center>
                  <img src="static/images/hw_events.png" class="center-image blend-img-background"/>
                </center>
              </div>

              <div class="content">
                <h2 class="title is-6">Event Description</h2>
                <center>
                  <img src="static/images/hw_ed_result.png" class="center-image blend-img-background"/>
                </center>
              </div>

              <div class="content">
                <h2 class="title is-6">Emotion Evaluation</h2>
                <center>
                  <img src="static/images/hw_ee_result.png" class="center-image blend-img-background"/>
                </center>
              </div>
            </div>

            <div class="content">
              <h2 class="title is-5">Dynamic Door</h2>
              <div class="content">
                <h2 class="title is-6">Event Images</h2>
                <center>
                  <img src="static/images/dd_events.png" class="center-image blend-img-background"/>
                </center>
              </div>

              <div class="content">
                <h2 class="title is-6">Event Description</h2>
                <center>
                  <img src="static/images/dd_ed_result.png" class="center-image blend-img-background"/>
                </center>
              </div>

              <div class="content">
                <h2 class="title is-6">Emotion Evaluation</h2>
                <center>
                  <img src="static/images/dd_ee_result.png" class="center-image blend-img-background"/>
                </center>
              </div>
            </div>
          </div> -->
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Experiments in Real-World Environment</h2>

          <div class="content">
            <h2 class="title is-4">Real-World Setup</h2>
            <center>
              <img src="static/images/209_realworld.png" class="center-image blend-img-background"/>
            </center>
            <div class="level-set has-text-justified">
              <p>
                To evaluate the scalability and applicability of our method in real-world settings, we first set up a real-world environment by placing objects such as a sofa, table, refrigerator, and microwave in the conference room at Seoul National University. We used the same language instructions as in the simulation and incorporated real humans and danger signs to replicate the three scenarios from the simulation.
              </p>
            </div>
          </div>
          
          <div class="content">
            <h2 class="title is-4">Hardware Setup</h2>
            <center>
              <img src="static/images/limbo_spec.png" class="center-image blend-img-background"/>
            </center>
            <div class="level-set has-text-justified">
              <p>
                For real-world experiments, we used a Unitree Go1 quadruped robot. The real-world robot is equipped with an Intel RealSense L515 RGB-D camera, a Velodyne VLP-16 3D LiDAR, and an Intel NUC 13 with i7 CPU for computation. For real-world experiments, the navigation algorithm runs on the Intel NUC, while all other algorithms are executed on a server with four RTX-4090 GPUs. The Intel NUC and the server communicate remotely via Wi-Fi.
              </p>
            </div>
          </div>

          <div class="content">
            <h2 class="title is-4">Quantitative Result</h2>
            <center>
              <img src="static/images/real_world_quan.png" class="center-image blend-img-background"/>
            </center>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
  <div class="row">
    <h2 class="title is-3">
      Prompts
    </h2>
    <p class="content has-text-justified">
        <a href="static/prompts/system_prompt.txt">System Prompt</a> |
        <a href="static/prompts/gs_prompt.txt">Goal Selector Prompt</a> |
        <a href="static/prompts/ed_prompt.txt">Event Descriptor Prompt</a> |
        <a href="static/prompts/ee_prompt.txt">Emotion Evaluator Prompt</a>
  </div>
  </div>
</section> -->

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{kim2025e2map,
  title={Project page for Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation},
  author={Chan Kim and Keonwoo Kim and Mintaek Oh and Hanbi Baek and Jiyang Lee and Donghwi Jung and Soojin Woo and Younkyung Woo and John Tucker and Roya Firoozi and Seung-Woo Seo and Mac Schwager and Seong-Woo Kim},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2025},
  organization={IEEE}
}
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->

<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
  This research was funded by the Korean Ministry of Land, Infrastructure and Transport (MOLIT) through the Smart City Innovative Talent Education Program and by the Korea Institute for Advancement of Technology (KIAT) under a MOTIE grant (P0020536). Additional support came from the Ministry of Education (MOE) and the National Research Foundation of Korea (NRF). K. Kim, D. Jung, and the corresponding author are affiliated with the Smart City Global Convergence program. Research facilities were provided by the Institute of Engineering Research at Seoul National University.  </div>
</section> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
